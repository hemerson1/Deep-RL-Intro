{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Aggregation (DAGGER) Model ##################################################################################\n",
    "\n",
    "# Use DAgger to play catcher game from PLE\n",
    "\n",
    "# It took 3 iterations before the model had mastered the game!!!\n",
    "\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1e39ebff113e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;31m# sets the wait between frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick_busy_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWAIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./Images/screenshot_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import numpy as np\n",
    "import random\n",
    "from ple import PLE\n",
    "import pygame\n",
    "import pickle \n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Dense \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.initializers import HeNormal, GlorotNormal\n",
    "\n",
    "from game import Catcher\n",
    "from utils import *\n",
    "\n",
    "#########################################################################\n",
    "# VARIABLES \n",
    "\n",
    "# WORKFLOW\n",
    "IS_TRAINING = False\n",
    "\n",
    "# ENVIRONMENT\n",
    "RENDER = True\n",
    "NB_FRAMES = 10000\n",
    "WAIT = 30 \n",
    "\n",
    "# TRAINING\n",
    "SAVE = False # True to save user data file\n",
    "TRAIN = False # True to train the model on data collected\n",
    "SELECTING = False # True if want to load a file different to the one collected\n",
    "CUSTOM_FILE = './Expert Data/2020-12-30_18-04-58.txt' # The custom file name \n",
    "RESUME = True # True if you want to load the current model and retrain\n",
    "\n",
    "# MODEL\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "# SUMMARY #############################################################\n",
    "# INPUTS: \n",
    "    # player x positon\n",
    "    # player velocity\n",
    "    # fruits x-position\n",
    "    # fruits y-position\n",
    "# OUTPUTS: \n",
    "    # prob of staying\n",
    "    # prob of turning right \n",
    "    # prob of turning left    \n",
    "######################################################################\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Want to match the current state to the action\n",
    "class Behavioural_Cloning():\n",
    "        \n",
    "    def __init__(self, lr=LEARNING_RATE):\n",
    "        self.lr = lr\n",
    "        self.model = self.create_model()        \n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((4,), input_shape=(4,1)))\n",
    "        model.add(Dense(100, activation='relu', kernel_initializer=HeNormal(seed=2)))\n",
    "        model.add(Dense(25, activation='relu', kernel_initializer=HeNormal(seed=2)))\n",
    "        model.add(Dense(3, activation='softmax', kernel_initializer=GlorotNormal(seed=2)))\n",
    "        model.compile(optimizer=Adam(learning_rate=self.lr), \n",
    "                       metrics=[\"accuracy\"],\n",
    "                       loss=\"categorical_crossentropy\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Learning from the expert policy maker\n",
    "if IS_TRAINING:\n",
    "    \n",
    "    # Initialise the environment\n",
    "    pygame.init()\n",
    "    game = Catcher(width=256, height=256)\n",
    "    rand_state = random.randint(1, 100)\n",
    "    game.rng = np.random.RandomState(rand_state)\n",
    "    game.clock = pygame.time.Clock()\n",
    "    game.screen = pygame.display.set_mode(game.getScreenDims(), 0, 32) # set screen to centre    \n",
    "    game.init()\n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    for iter in range(NB_FRAMES):\n",
    "        \n",
    "        # sets the wait between frames\n",
    "        dt = game.clock.tick_busy_loop(WAIT)\n",
    "        \n",
    "        state = game.getGameState()\n",
    "        action = game.getKeyPressed()        \n",
    "        \n",
    "        # take a step \n",
    "        game.step(dt)\n",
    "        pygame.display.update() \n",
    "        \n",
    "        # save the data to the lists \n",
    "        inputs.append(state)\n",
    "        outputs.append(action)\n",
    "        \n",
    "        if game.game_over() or iter > 1000: \n",
    "            pygame.quit()\n",
    "            \n",
    "            # how many rounds did the user run for? \n",
    "            print('Ran for: ' + str(iter) + ' frames' )\n",
    "            \n",
    "            dt_string = None            \n",
    "            if SAVE:             \n",
    "                # get current time + date \n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "                # save the file to pickle\n",
    "                with open(\"./Expert Data/\" + dt_string + \".txt\", \"wb\") as data1:\n",
    "                    pickle.dump([inputs, outputs], data1)\n",
    "            \n",
    "            if TRAIN and SAVE:\n",
    "                print('Training the model')\n",
    "                \n",
    "                if SELECTING:\n",
    "                    dataSource = CUSTOM_FILE                  \n",
    "                \n",
    "                else:     \n",
    "                    dataSource = './Expert Data/' + dt_string + '.txt' \n",
    "                 \n",
    "                # Unpickling\n",
    "                with open(dataSource, \"rb\") as data1:   \n",
    "                    expert_data = pickle.load(data1)\n",
    "\n",
    "                # Format the dataset     \n",
    "                inputs = expert_data[0] # the inputs \n",
    "                outputs = np.array(expert_data[1]).astype(str) # the outputs\n",
    "                \n",
    "                # get the split of values\n",
    "                unique, counts = np.unique(outputs, return_counts=True)\n",
    "                print(dict(zip(unique, counts)))\n",
    "                \n",
    "                # One hot encode data    \n",
    "                label_encoder = LabelEncoder()\n",
    "                onehot_encoder = OneHotEncoder(sparse=False)    \n",
    "                integer_encoded = label_encoder.fit_transform(outputs)\n",
    "                integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)    \n",
    "                y = onehot_encoder.fit_transform(integer_encoded).reshape(3, len(integer_encoded))\n",
    "\n",
    "                # import from utils -> perform preprocessing\n",
    "                X = process_states(inputs)\n",
    "\n",
    "                # split the data into training, validation and testing data\n",
    "                # test: 20%, train: 60%, validation: 20%\n",
    "                # reshaped again to utilise spliting function correctly\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X.reshape(X.shape[1], 4), y.reshape(y.shape[1], 3), test_size=0.2, random_state=42) \n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "                \n",
    "                # load in the correct model\n",
    "                if os.path.isfile('model_imitation_dag_1.h5') and RESUME:\n",
    "                    model = load_model('model_imitation_dag_1.h5')\n",
    "                else:\n",
    "                    model = Behavioural_Cloning().model  \n",
    "                    \n",
    "                # add some class weights\n",
    "                class_weight = {0: 1., 1: 25., 2: 25.}\n",
    "\n",
    "                # train the model using the selected data source \n",
    "                train_res = model.fit(X_train, y_train,\n",
    "                                      validation_data=(X_val, y_val),\n",
    "                                      epochs=EPOCHS,\n",
    "                                      class_weight=class_weight,\n",
    "                                      verbose=2) \n",
    "                \n",
    "                test_res = model.evaluate(X_test, y_test,\n",
    "                                          verbose=2)  \n",
    "                \n",
    "                print('Training Complete')\n",
    "                \n",
    "                # Save the model                \n",
    "                #model.save('model_imitation_dag_1.h5')\n",
    "                \n",
    "                print('Model Saved')\n",
    "                \n",
    "            break          \n",
    "           \n",
    "        \n",
    "        \n",
    "# Testing the apprentice policy maker\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Initialise the environment\n",
    "    pygame.init()\n",
    "    game = Catcher(width=256, height=256)    \n",
    "    rand_state = random.randint(1, 100)\n",
    "    game.rng = np.random.RandomState(rand_state)\n",
    "    game.clock = pygame.time.Clock()\n",
    "    game.screen = pygame.display.set_mode(game.getScreenDims(), 0, 32) # set screen to centre   \n",
    "    env = PLE(game, fps=60, display_screen=RENDER)\n",
    "    env.init()\n",
    "    \n",
    "    # intialise the model\n",
    "    model = load_model('./Weights/model_imitation_dag_1.h5')\n",
    "    \n",
    "    # get the possible actions\n",
    "    possible_actions = game.actions \n",
    "    \n",
    "    # for some reason breaks without this    \n",
    "    env.act(env.NOOP)\n",
    "    \n",
    "    # collect the states and the perfect actions from the policy maker \n",
    "    inputs = []\n",
    "    policy_outputs = []\n",
    "    \n",
    "    for iter in range(NB_FRAMES):\n",
    "        \n",
    "        # sets the wait between frames\n",
    "        dt = game.clock.tick_busy_loop(WAIT)   \n",
    "        \n",
    "        # is the game over?\n",
    "        if env.game_over(): \n",
    "            pygame.quit()\n",
    "            \n",
    "            # how many rounds did the user run for?             \n",
    "            print('Ran for: ' + str(iter) + ' frames' )  \n",
    "            \n",
    "            dt_string = None            \n",
    "            if SAVE:             \n",
    "                # get current time + date \n",
    "                now = datetime.now()\n",
    "                dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "                # save the file to pickle\n",
    "                with open(\"./Expert Data/\" + dt_string + \"-Perfect.txt\", \"wb\") as data1:\n",
    "                    pickle.dump([inputs, policy_outputs], data1)\n",
    "            \n",
    "            if TRAIN and SAVE:\n",
    "                print('Training the model')\n",
    "                \n",
    "                if SELECTING:\n",
    "                    dataSource = CUSTOM_FILE                  \n",
    "                \n",
    "                else:     \n",
    "                    dataSource = './Expert Data/' + dt_string + '-Perfect.txt' \n",
    "                 \n",
    "                # Unpickling\n",
    "                with open(dataSource, \"rb\") as data1:   \n",
    "                    expert_data = pickle.load(data1)\n",
    "\n",
    "                # Format the dataset     \n",
    "                inputs = expert_data[0] # the inputs \n",
    "                outputs = np.array(expert_data[1]).astype(str) # the outputs\n",
    "                \n",
    "                # get the split of values\n",
    "                unique, counts = np.unique(outputs, return_counts=True)\n",
    "                unique_values = dict(zip(unique, counts))\n",
    "                print(unique_values)\n",
    "                \n",
    "                # One hot encode data    \n",
    "                label_encoder = LabelEncoder()\n",
    "                onehot_encoder = OneHotEncoder(sparse=False)    \n",
    "                integer_encoded = label_encoder.fit_transform(outputs)\n",
    "                integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)    \n",
    "                y = onehot_encoder.fit_transform(integer_encoded).reshape(3, len(integer_encoded))\n",
    "\n",
    "                # import from utils -> perform preprocessing\n",
    "                X = process_states(inputs)\n",
    "\n",
    "                # split the data into training, validation and testing data\n",
    "                # test: 20%, train: 60%, validation: 20%\n",
    "                # reshaped again to utilise spliting function correctly\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X.reshape(X.shape[1], 4), y.reshape(y.shape[1], 3), test_size=0.2, random_state=42) \n",
    "                X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "                \n",
    "                # load in the correct model\n",
    "                if os.path.isfile('./Weights/model_imitation_dag_1.h5') and RESUME:\n",
    "                    model = load_model('./Weights/model_imitation_dag_1.h5')\n",
    "                else:\n",
    "                    model = Behavioural_Cloning().model \n",
    "                    \n",
    "                max_class = max([unique_values['None'], unique_values['left'], unique_values['right']])\n",
    "                \n",
    "                # add some class weights\n",
    "                class_weight = {0: max_class/unique_values['None'],\n",
    "                                1: max_class/unique_values['left'],\n",
    "                                2: max_class/unique_values['left']}\n",
    "\n",
    "                # train the model using the selected data source \n",
    "                train_res = model.fit(X_train, y_train,\n",
    "                                      validation_data=(X_val, y_val),\n",
    "                                      epochs=EPOCHS,\n",
    "                                      class_weight=class_weight,\n",
    "                                      verbose=2) \n",
    "                \n",
    "                test_res = model.evaluate(X_test, y_test,\n",
    "                                          verbose=2)  \n",
    "                \n",
    "                print('Training Complete')\n",
    "                \n",
    "                # Save the model                \n",
    "                model.save('model_imitation_dag_1.h5')\n",
    "                \n",
    "                print('Model Saved')\n",
    "                \n",
    "            break        \n",
    "        \n",
    "        # get the current game state\n",
    "        state = env.getGameState() \n",
    "        \n",
    "        #  get ideal action\n",
    "        perfect_action = get_teacher_action(state)         \n",
    "        \n",
    "        # decide the current action \n",
    "        # state must be in [] for iteration in process states  \n",
    "        # using model instead of model.predict is so much faster!\n",
    "        action_prob = model(process_states([state]).reshape(1, len(state)), training=False) \n",
    "        selected_action = np.argmax(action_prob[0])\n",
    "        \n",
    "        env.act(possible_actions[selected_action])        \n",
    "        \n",
    "        # save the data to the lists \n",
    "        inputs.append(state)\n",
    "        policy_outputs.append(perfect_action)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
