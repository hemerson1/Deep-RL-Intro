{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actor Critic Model - testing on the half cheetah environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controlled-ecology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "Episode finished after 31 timesteps\n"
     ]
    }
   ],
   "source": [
    "## IMPORTANT: In order for mujoco to work must run it from the command line, i.e. jupyter lab \n",
    "import gym\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# ENVIRONMENT SUMMARY: \n",
    "# -> Continuous action space\n",
    "\n",
    "# Observation -> 17 items\n",
    "# These correspond to the position, angle and speed of the joints\n",
    "\n",
    "# Action -> 6 items\n",
    "# These correspond to 6 actuators in the cheetah \n",
    "# Value specifies the torque\n",
    "\n",
    "# Reward -> 1 item\n",
    "# Reward is proportional to the velocity of the cheetah\n",
    "# Reward is subtracted by a ctrl cost which I think is proportional to the applied torque.\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# TODO: \n",
    "# - Add the correct layer dimensions\n",
    "\n",
    "# VARIABLES:\n",
    "NB_FRAMES = 10000\n",
    "\n",
    "# returns the value function \n",
    "class Critic(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(2048,activation='relu')\n",
    "        self.d2 = Dense(1536,activation='relu')\n",
    "        self.v = Dense(1, activation = None)\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.d1(input_data)\n",
    "        x = self.d2(x)\n",
    "        v = self.v(x)\n",
    "        return v\n",
    "\n",
    "# returns the action\n",
    "class Actor(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(2048,activation='relu')\n",
    "        self.d2 = Dense(1536,activation='relu')\n",
    "        self.a = Dense(6,activation='softmax')\n",
    "\n",
    "    def call(self, input_data):\n",
    "        x = self.d1(input_data)\n",
    "        x = self.d2(x)\n",
    "        a = self.a(x)\n",
    "        return a\n",
    "        \n",
    "        \n",
    "\n",
    "# intitialise the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "for frame in range(NB_FRAMES):\n",
    "    env.render()\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(frame+1))\n",
    "        break\n",
    "            \n",
    "# close the environment\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
