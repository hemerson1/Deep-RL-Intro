{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Networks - Getting a Deep Q network to play Ms Pacman\n",
    "\n",
    "# For image preprocessing used: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "# For NN structure used: https://github.com/yxu1168/Reinforcement-Learning-DQN-for-ATARI-s-Pong-Game---TensorFlow-2.0-Keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SANITY TEST ##############################################################\n",
    "\n",
    "# ENVIRONMENT SUMMARY: -> Pong-v0 \n",
    "# -> Discrete action space\n",
    "# -> The easiest atari game with discrete action space\n",
    "\n",
    "# Observation -> 1 item\n",
    "# An array of shape (210, 160, 3) corresponding to the screen image\n",
    "\n",
    "# Action -> 6 items\n",
    "# [‘NOOP’, ‘FIRE’, ‘RIGHT’, ‘LEFT’, ‘RIGHTFIRE’, ‘LEFTFIRE’]\n",
    "# NOOP = FIRE, RIGHT = RIGHTFIRE, LEFT = LEFTFIRE\n",
    "# so really only 3 actual actions\n",
    "\n",
    "# Reward -> 1 item\n",
    "# One point is subtracted for conceeding and one point is game for winning\n",
    "# For a total of 21 games\n",
    "\n",
    "### MAIN #######################################################################\n",
    "\n",
    "# ENVIRONMENT SUMMARY: -> MsPacman-v0\n",
    "# -> Discrete action space\n",
    "\n",
    "# Observation -> 17 items\n",
    "# \n",
    "\n",
    "# Action -> 6 items\n",
    "#\n",
    "\n",
    "# Reward -> 1 item\n",
    "#\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "import gym \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from collections import deque\n",
    "\n",
    "# VARIABLES: ####################################################################\n",
    "\n",
    "TOTAL_EPISODES = 10\n",
    "NB_FRAMES = 1000\n",
    "ACTION_SPACE = 6\n",
    "ENV_NAME = 'Pong-v0'\n",
    "\n",
    "### FUNCTIONS ###################################################################\n",
    "\n",
    "# from Karpathy pong from pixels\n",
    "# 210x160x3 uint8 frame into 6400 (80x80) 1D float vector\n",
    "def PreProcessPong(I):\n",
    "    \n",
    "    # crop\n",
    "    I = I[35:195] \n",
    "    \n",
    "    # downsample by factor of 2\n",
    "    I = I[::2,::2,0] \n",
    "    \n",
    "    # erase background (background type 1)\n",
    "    I[I == 144] = 0 \n",
    "    \n",
    "    # erase background (background type 2)\n",
    "    I[I == 109] = 0 \n",
    "    \n",
    "    # everything else (paddles, ball) just set to 1\n",
    "    I[I != 0] = 1 \n",
    "    \n",
    "    # remove ravel to input into Conv\n",
    "    return I.astype(np.float)\n",
    "\n",
    "    \n",
    "# the DQN\n",
    "class DeepQNetwork:\n",
    "    def __init__(self, num_states, num_actions, gamma, batch_size, lr):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = Adam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = Q_Model(num_states, num_actions, self.optimizer)\n",
    "        self.replay_length = 20000\n",
    "        self.experience = deque(maxlen=self.replay_length)\n",
    "    \n",
    "    # act based on the policy maker\n",
    "    def act(self, state, epsilon):\n",
    "        \n",
    "        # take a random action\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        \n",
    "        # select the action that maximises the Q value\n",
    "        else:\n",
    "            Q_values = self.model.Q_model.predict(state[np.newaxis])\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    # sample from the experience relay\n",
    "    def sample_experiences(self):\n",
    "        \n",
    "        # sample indices randomly from the replay\n",
    "        indices = np.random.randint(self.replay_length , size=self.batch_size)\n",
    "        \n",
    "        # extract the batch\n",
    "        batch = [replay_memory[index] for index in indices]\n",
    "        \n",
    "        # extract the states, actions, rewards, previous states & dones from the batch\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            np.array([experience[field_index] for experience in batch])\n",
    "            for field_index in range(5)]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "        \n",
    "    \n",
    "# Takes a state and returns Q values for each possible action\n",
    "class Q_Model:\n",
    "    def __init__(self, num_states, num_actions, optimizer):\n",
    "        \n",
    "        self.action_space = num_actions\n",
    "        X_input = Input(shape=(num_states))  \n",
    "        \n",
    "        # Convolutional Layers\n",
    "        X = Conv2D(16, kernel_size=8, strides=4, activation=\"relu\")(X_input)\n",
    "        X = Conv2D(32, kernel_size=4, strides=2, activation=\"relu\")(X)\n",
    "        X = Conv2D(32, kernel_size=3, strides=1, activation=\"relu\")(X)\n",
    "        X = Flatten()(X)\n",
    "        \n",
    "        # State -> Q values\n",
    "        X = Dense(64, activation=\"relu\")(X)\n",
    "        X = Dense(64, activation=\"relu\")(X)\n",
    "        output = Dense(self.action_space)(X)\n",
    "\n",
    "        self.Q_Net = Model(inputs = X_input, outputs = output)\n",
    "        self.Q_Net.compile(loss=mean_squared_error, optimizer=optimizer)\n",
    "        \n",
    "        print(self.Q_Net.summary())\n",
    "        \n",
    "        \n",
    "        \n",
    "#################################################################################\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "\n",
    "agent = DeepQNetwork(num_states = (80, 80, 1),\n",
    "                     num_actions = 2,\n",
    "                     gamma = 0.99,\n",
    "                     batch_size = 32,\n",
    "                     lr = 1e-03)        \n",
    "\n",
    "# Initialise the environment   \n",
    "obs = env.reset()  \n",
    "prev_x = None \n",
    "    \n",
    "for e in range(1, TOTAL_EPISODES + 1):\n",
    "\n",
    "    print('Starting Episode {}'.format(e)) \n",
    "\n",
    "    for f in range(NB_FRAMES):\n",
    "        \n",
    "        # from Karpathy -> take the difference of two images\n",
    "        s = PreProcessPong(obs)\n",
    "        \n",
    "        # sample action \n",
    "        a = env.action_space.sample()\n",
    "        \n",
    "        # perform action\n",
    "        obs, r, done, _ = env.step(a) \n",
    "\n",
    "        # when episode ends\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "# close the environment\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
