{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Networks - Getting a Deep Q network to play Ms Pacman\n",
    "\n",
    "# Used code from: https://github.com/pythonlessons/Reinforcement_Learning/blob/master/07_Pong-reinforcement-learning_DQN_CNN/Pong-v0_DQN_CNN_TF2.py\n",
    "\n",
    "### SANITY TEST ##############################################################\n",
    "\n",
    "# ENVIRONMENT SUMMARY: -> Pong-v0 \n",
    "# -> Discrete action space\n",
    "# -> The easiest atari game with discrete action space\n",
    "\n",
    "# Observation -> 1 item\n",
    "# An array of shape (210, 160, 3) corresponding to the screen image\n",
    "\n",
    "# Action -> 6 items\n",
    "# [‘NOOP’, ‘FIRE’, ‘RIGHT’, ‘LEFT’, ‘RIGHTFIRE’, ‘LEFTFIRE’]\n",
    "# NOOP = FIRE, RIGHT = RIGHTFIRE, LEFT = LEFTFIRE\n",
    "# so really only 3 actual actions\n",
    "\n",
    "# Reward -> 1 item\n",
    "# -1 for conceeding and +1 for scoring\n",
    "\n",
    "### MAIN #######################################################################\n",
    "\n",
    "# ENVIRONMENT SUMMARY: -> MsPacman-v0\n",
    "# -> Discrete action space\n",
    "\n",
    "# Observation -> 17 items\n",
    "# \n",
    "\n",
    "# Action -> 6 items\n",
    "#\n",
    "\n",
    "# Reward -> 1 item\n",
    "#\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", input_shape=input_shape, activation=\"relu\")(X)\n",
    "    X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\")(X)\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(0)  \n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 25000\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.epsilon = 1.0  \n",
    "        self.epsilon_min = 0.02 \n",
    "        self.epsilon_decay = 0.00002  \n",
    "        \n",
    "        self.batch_size = 32        \n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4        \n",
    "        self.update_model_steps = 1000\n",
    "        \n",
    "        self.state_size = (self.ROWS, self.COLS,self.REM_STEP)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size)\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        return\n",
    "    \n",
    "    def act(self, state, decay_step):    \n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (1-self.epsilon_decay)\n",
    "        explore_probability = self.epsilon\n",
    "            \n",
    "        if explore_probability > np.random.rand():\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            return np.argmax(self.model(state.reshape(1,80,80,4))), explore_probability\n",
    "              \n",
    "    def replay(self):        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "            \n",
    "        target = np.array(self.model(state))        \n",
    "        target_val = self.target_model(next_state)\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])         \n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)    \n",
    "\n",
    "    def GetImage(self, frame):        \n",
    "        #self.env.render()              \n",
    "        frame_cropped = frame[35:195:2, ::2,:]        \n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 2)\n",
    "        self.image_memory[:,:,0] = new_frame\n",
    "        \n",
    "        return self.image_memory.reshape(80,80,4)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        max_average = -21.0\n",
    "        saved_scores = []\n",
    "        \n",
    "        for e in range(1, self.EPISODES + 1):\n",
    "            \n",
    "            print('Starting Episode: ', e)\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "        \n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done:                    \n",
    "                    print('Episode: {} Score: {} Epsilon: {}'.format(e, score, self.epsilon))                     \n",
    "                    saved_scores.append(score)        \n",
    "                    \n",
    "                    if e % 10 == 0:\n",
    "\n",
    "                        # save results\n",
    "                        with open(\"./Results/rewards-ep-\" + str(e) + \".txt\", \"wb\") as file:\n",
    "                            pickle.dump(saved_scores, file) \n",
    "\n",
    "                        # save model\n",
    "                        self.save('./Models/DQN_Model.h5')\n",
    "\n",
    "                        # save the replay\n",
    "                        with open(\"./Replays/replay1.txt\", \"wb\") as file:\n",
    "                            pickle.dump(self.image_memory, file)\n",
    "                    \n",
    "                            \n",
    "                # update the models            \n",
    "                if decay_step % 1000 == 0:\n",
    "                    self.update_target_model()\n",
    "                    \n",
    "                if decay_step % 1 == 0:\n",
    "                    self.replay()\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'Pong-v0'\n",
    "    agent = DQNAgent(env_name)\n",
    "    #agent.load('/content/drive/MyDrive/Colab Notebooks/Models/DQN_Model-67-ep.h5')\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(\"./Results/rewards-ep-67.txt\", \"rb\") as file:\n",
    "    a = pickle.load(file)  \n",
    "    \n",
    "\n",
    "x1 = np.array(range(1,68))\n",
    "y1 = a\n",
    "\"\"\"x2 = np.array(range(1, 3001))\n",
    "y2 = a\n",
    "\"\"\"\n",
    "#plt.plot(x2,y2)\n",
    "plt.plot(x1,y1)\n",
    "plt.xlim(0, 67)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.ylim(-22, -15)\n",
    "plt.grid(alpha=1.0)\n",
    "#plt.savefig('./Images/Cheet4000ep.png')\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
