{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Networks - Getting a Deep Q network to play Ms Pacman\n",
    "\n",
    "# Used code from: https://github.com/pythonlessons/Reinforcement_Learning/blob/master/07_Pong-reinforcement-learning_DQN_CNN/Pong-v0_DQN_CNN_TF2.py\n",
    "# Used code from: https://github.com/ShivankYadav/LunarLander-using-DQN/blob/master/model.py\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "def OurModel(input_shape, action_space, lr):\n",
    "    X_input = Input(shape=(input_shape,))\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    X = Dense(128, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):\n",
    "        \n",
    "        # Environment\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_size = 8\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Memory\n",
    "        memory_size = 100000 # 2500\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        # Exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01 \n",
    "        self.epsilon_decay = 0.995    \n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99        \n",
    "        self.batch_size = 64\n",
    "        self.lr = 5e-4 # 1e-4\n",
    "        self.tau = 1e-3 \n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size, lr= self.lr)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size, lr= self.lr)\n",
    "        \n",
    "    def update_target_model(self):        \n",
    "        for t, e in zip(self.target_model.trainable_variables, self.model.trainable_variables):\n",
    "            t.assign(t * (1 - self.tau) + e * self.tau)\n",
    "        return\n",
    "    \n",
    "    # act based on agent\n",
    "    def act(self, state):                \n",
    "        if self.epsilon > np.random.rand():\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model(state))\n",
    "        \n",
    "    def replay(self):        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "                \n",
    "        target = np.array(self.model(state))  \n",
    "        #target_next = self.model(next_state)\n",
    "        target_val = self.target_model(next_state)\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])         \n",
    "                \n",
    "                # DDQN\n",
    "                # a = np.argmax(target_next[i])\n",
    "                # target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]                 \n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "        \n",
    "        # Soft update of the model\n",
    "        self.update_target_model()\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)    \n",
    "        self.target_model.load_weights(name)    \n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)    \n",
    "\n",
    "    def Normalise(self, frame):        \n",
    "        self.env.render()          \n",
    "                \n",
    "        \"\"\"# convert to float32\n",
    "        frame = np.array(frame, dtype=np.float32)        \n",
    "        mean = np.mean(frame, dtype=np.float32)\n",
    "        std = np.std(frame, dtype=np.float32)\n",
    "        \n",
    "        frame -= mean\n",
    "        frame /= std \"\"\"\n",
    "        \n",
    "        return np.expand_dims(frame, axis=0)\n",
    "    \n",
    "    # reset the environment\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        state = self.Normalise(frame)\n",
    "        return state\n",
    "    \n",
    "    # perform the next step\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.Normalise(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        saved_scores = []  \n",
    "        checked = False\n",
    "        eps = 1.0\n",
    "        eps_decay = 0.995\n",
    "        \n",
    "        \n",
    "        for e in range(1, self.EPISODES + 1):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "        \n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                self.memory.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                \n",
    "                # announce when buffer full\n",
    "                if len(self.memory) > 99990 and not checked:\n",
    "                    print('Buffer full!')\n",
    "                    checked = True                    \n",
    "\n",
    "                if done:                    \n",
    "                    print('Episode: {} Score: {} Epsilon: {}'.format(e, score, self.epsilon))                     \n",
    "                    saved_scores.append(score)    \n",
    "                    \n",
    "                    if e % 100 == 0:\n",
    "\n",
    "                        # save results\n",
    "                        with open(\"./Results/rewards-ep-\" + str(e) + \".txt\", \"wb\") as file:\n",
    "                            pickle.dump(saved_scores, file) \n",
    "\n",
    "                        # save model\n",
    "                        self.save('./Models/DQN_Model.h5')\n",
    "\n",
    "                        # save the replay\n",
    "                        with open(\"./Replays/replay1.txt\", \"wb\") as file:\n",
    "                            pickle.dump(self.memory, file)                    \n",
    "                    \n",
    "                if decay_step % 4 == 0:\n",
    "                    self.replay()\n",
    "                \n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon) \n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "        \n",
    "with open(\"./Replays/replay1.txt\", \"rb\") as file:\n",
    "    memory = pickle.load(file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # env_name = 'MsPacman-ram-v0'    \n",
    "    env_name = 'LunarLander-v2'  \n",
    "    agent = DQNAgent(env_name)\n",
    "    agent.memory = memory\n",
    "    #agent.load('./Models/DDQN_Model.h5')\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(\"./Results/rewards-ep-800.txt\", \"rb\") as file:\n",
    "    a = pickle.load(file)  \n",
    "\n",
    "\"\"\"\n",
    "with open(\"./Results/rewards-ep-200-lr0.005.txt\", \"rb\") as file:\n",
    "    b = pickle.load(file)  \n",
    "    \n",
    "with open(\"./Results/rewards-ep-200-lr0.025.txt\", \"rb\") as file:\n",
    "    c = pickle.load(file)  \n",
    "    \n",
    "with open(\"./Results/rewards-ep-200-lr2.5e-05.txt\", \"rb\") as file:\n",
    "    d = pickle.load(file)  \n",
    "    \n",
    "with open(\"./Results/rewards-ep-200-lr2.5e-06.txt\", \"rb\") as file:\n",
    "    e = pickle.load(file)  \n",
    "    \n",
    "with open(\"./Results/rewards-ep-200-lr2.5e-07.txt\", \"rb\") as file:\n",
    "    f = pickle.load(file)  \n",
    "   \"\"\" \n",
    "meana = []       \n",
    "for i in range(len(a)):    \n",
    "    if i % 100 == 0:\n",
    "        meana.append(np.mean(a[i - 10: i]))   \n",
    "    \n",
    "\"\"\"meanb = []       \n",
    "for i in range(len(a)):    \n",
    "    if i % 10 == 0:\n",
    "        meanb.append(np.mean(b[i - 10: i]))   \n",
    "        \n",
    "meanc = []       \n",
    "for i in range(len(a)):    \n",
    "    if i % 10 == 0:\n",
    "        meanc.append(np.mean(c[i - 10: i]))   \n",
    "        \n",
    "meand = []       \n",
    "for i in range(len(a)):    \n",
    "    if i % 10 == 0:\n",
    "        meand.append(np.mean(d[i - 10: i]))   \n",
    "\n",
    "meane = []       \n",
    "for i in range(len(a)):    \n",
    "    if i % 10 == 0:\n",
    "        meane.append(np.mean(e[i - 10: i]))   \n",
    "\n",
    "meanf = []       \n",
    "for i in range(len(a)):    \n",
    "    if i % 10 == 0:\n",
    "        meanf.append(np.mean(f[i - 10: i])) \"\"\"  \n",
    "\n",
    "x1 = np.array(range(1,9)) * 100\n",
    "y1 = meana\n",
    "\"\"\"y2 = meanb\n",
    "y3 = meanc\n",
    "y4 = meand\n",
    "y5 = meane\n",
    "y6 = meanf\"\"\"\n",
    "\n",
    "plt.plot(x1,y1)\n",
    "\"\"\"plt.plot(x1,y2)\n",
    "plt.plot(x1,y3)\n",
    "plt.plot(x1,y4)\n",
    "plt.plot(x1,y5)\n",
    "plt.plot(x1,y6)\"\"\"\n",
    "plt.xlim(10, 1000)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(alpha=1.0)\n",
    "# plt.legend(('0.00025','0.005', '0.025', '2.5e-05', '2.5e-06', '2.5e-07'))\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
